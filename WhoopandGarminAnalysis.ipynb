{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consolidate CSV & JSON Files from Many Folders to One**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whoop Data - CSV & JSON to RawWhoopData -> ConsoldiatedWhoopData\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "\n",
    "# ----------- Configuration Section -----------\n",
    "source_folder_name = \"RawWhoopData\"\n",
    "destination_folder_name = \"ConsolidatedWhoopData\"\n",
    "# ----------------------------------------------\n",
    "\n",
    "def calculate_file_hash(file_path):\n",
    "    \"\"\"Calculate SHA256 hash of a file.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def find_folder(base_folder, target_folder_name):\n",
    "    \"\"\"Recursively find a folder with a specific name.\"\"\"\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        if target_folder_name in dirs:\n",
    "            return os.path.join(root, target_folder_name)\n",
    "    return None\n",
    "\n",
    "def move_csv_and_json_files(source, destination):\n",
    "    csv_files_moved = 0\n",
    "    json_files_moved = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(source):\n",
    "        for file in files:\n",
    "            file_clean = file.strip().lower()\n",
    "            if file_clean.endswith('.csv') or file_clean.endswith('.json'):\n",
    "                source_path = os.path.join(root, file)\n",
    "                destination_path = os.path.join(destination, file)\n",
    "\n",
    "                if not os.path.exists(destination):\n",
    "                    os.makedirs(destination)\n",
    "\n",
    "                # If a file with the same name exists, rename it\n",
    "                if os.path.exists(destination_path):\n",
    "                    base, ext = os.path.splitext(file)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(destination_path):\n",
    "                        new_filename = f\"{base}_{counter}{ext}\"\n",
    "                        destination_path = os.path.join(destination, new_filename)\n",
    "                        counter += 1\n",
    "\n",
    "                shutil.move(source_path, destination_path)\n",
    "\n",
    "                if file_clean.endswith('.csv'):\n",
    "                    csv_files_moved += 1\n",
    "                elif file_clean.endswith('.json'):\n",
    "                    json_files_moved += 1\n",
    "\n",
    "    return csv_files_moved, json_files_moved\n",
    "\n",
    "def delete_subfolders_in_source(source):\n",
    "    subfolders = [os.path.join(source, d) for d in os.listdir(source) if os.path.isdir(os.path.join(source, d))]\n",
    "    for folder in subfolders:\n",
    "        shutil.rmtree(folder)\n",
    "    return len(subfolders)\n",
    "\n",
    "def remove_duplicate_files(destination):\n",
    "    seen_hashes = {}\n",
    "    duplicate_csv = 0\n",
    "    duplicate_json = 0\n",
    "\n",
    "    for file in os.listdir(destination):\n",
    "        file_path = os.path.join(destination, file)\n",
    "        file_clean = file.strip().lower()\n",
    "\n",
    "        if os.path.isfile(file_path) and (file_clean.endswith('.csv') or file_clean.endswith('.json')):\n",
    "            file_hash = calculate_file_hash(file_path)\n",
    "            if file_hash in seen_hashes:\n",
    "                # Duplicate found\n",
    "                ext = os.path.splitext(file)[1].lower()\n",
    "                os.remove(file_path)\n",
    "\n",
    "                if ext == \".csv\":\n",
    "                    duplicate_csv += 1\n",
    "                elif ext == \".json\":\n",
    "                    duplicate_json += 1\n",
    "            else:\n",
    "                seen_hashes[file_hash] = file_path\n",
    "\n",
    "    return duplicate_csv, duplicate_json\n",
    "\n",
    "# ----------- Main Execution -----------\n",
    "# Get the current folder where the script is running\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Find source and destination folders dynamically\n",
    "source_folder = find_folder(current_dir, source_folder_name)\n",
    "destination_folder = find_folder(current_dir, destination_folder_name)\n",
    "\n",
    "if not source_folder:\n",
    "    print(f\"Source folder '{source_folder_name}' not found.\")\n",
    "elif not destination_folder:\n",
    "    print(f\"Destination folder '{destination_folder_name}' not found.\")\n",
    "else:\n",
    "    # Move files\n",
    "    csv_moved, json_moved = move_csv_and_json_files(source_folder, destination_folder)\n",
    "\n",
    "    # Delete subfolders\n",
    "    subfolders_deleted = delete_subfolders_in_source(source_folder)\n",
    "\n",
    "    # Remove duplicate files\n",
    "    duplicate_csv_deleted, duplicate_json_deleted = remove_duplicate_files(destination_folder)\n",
    "\n",
    "    # Final Output\n",
    "    print(f\"CSV Files Moved: {csv_moved}\")\n",
    "    print(f\"JSON Files Moved: {json_moved}\")\n",
    "    print(f\"Source Sub Folders Deleted: {subfolders_deleted}\")\n",
    "    print(f\"Duplicate CSVs Deleted: {duplicate_csv_deleted}\")\n",
    "    print(f\"Duplicate JSONs Deleted: {duplicate_json_deleted}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garmin Data - CSV & JSON to RawWhoopData -> ConsoldiatedWhoopData\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "\n",
    "# ----------- Configuration Section -----------\n",
    "source_folder_name = \"RawGarminData\"\n",
    "destination_folder_name = \"ConsolidatedGarminData\"\n",
    "# ----------------------------------------------\n",
    "\n",
    "def calculate_file_hash(file_path):\n",
    "    \"\"\"Calculate SHA256 hash of a file.\"\"\"\n",
    "    hasher = hashlib.sha256()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def find_folder(base_folder, target_folder_name):\n",
    "    \"\"\"Recursively find a folder with a specific name.\"\"\"\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        if target_folder_name in dirs:\n",
    "            return os.path.join(root, target_folder_name)\n",
    "    return None\n",
    "\n",
    "def move_csv_and_json_files(source, destination):\n",
    "    csv_files_moved = 0\n",
    "    json_files_moved = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(source):\n",
    "        for file in files:\n",
    "            file_clean = file.strip().lower()\n",
    "            if file_clean.endswith('.csv') or file_clean.endswith('.json'):\n",
    "                source_path = os.path.join(root, file)\n",
    "                destination_path = os.path.join(destination, file)\n",
    "\n",
    "                if not os.path.exists(destination):\n",
    "                    os.makedirs(destination)\n",
    "\n",
    "                # If a file with the same name exists, rename it\n",
    "                if os.path.exists(destination_path):\n",
    "                    base, ext = os.path.splitext(file)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(destination_path):\n",
    "                        new_filename = f\"{base}_{counter}{ext}\"\n",
    "                        destination_path = os.path.join(destination, new_filename)\n",
    "                        counter += 1\n",
    "\n",
    "                shutil.move(source_path, destination_path)\n",
    "\n",
    "                if file_clean.endswith('.csv'):\n",
    "                    csv_files_moved += 1\n",
    "                elif file_clean.endswith('.json'):\n",
    "                    json_files_moved += 1\n",
    "\n",
    "    return csv_files_moved, json_files_moved\n",
    "\n",
    "def delete_subfolders_in_source(source):\n",
    "    subfolders = [os.path.join(source, d) for d in os.listdir(source) if os.path.isdir(os.path.join(source, d))]\n",
    "    for folder in subfolders:\n",
    "        shutil.rmtree(folder)\n",
    "    return len(subfolders)\n",
    "\n",
    "def remove_duplicate_files(destination):\n",
    "    seen_hashes = {}\n",
    "    duplicate_csv = 0\n",
    "    duplicate_json = 0\n",
    "\n",
    "    for file in os.listdir(destination):\n",
    "        file_path = os.path.join(destination, file)\n",
    "        file_clean = file.strip().lower()\n",
    "\n",
    "        if os.path.isfile(file_path) and (file_clean.endswith('.csv') or file_clean.endswith('.json')):\n",
    "            file_hash = calculate_file_hash(file_path)\n",
    "            if file_hash in seen_hashes:\n",
    "                # Duplicate found\n",
    "                ext = os.path.splitext(file)[1].lower()\n",
    "                os.remove(file_path)\n",
    "\n",
    "                if ext == \".csv\":\n",
    "                    duplicate_csv += 1\n",
    "                elif ext == \".json\":\n",
    "                    duplicate_json += 1\n",
    "            else:\n",
    "                seen_hashes[file_hash] = file_path\n",
    "\n",
    "    return duplicate_csv, duplicate_json\n",
    "\n",
    "# ----------- Main Execution -----------\n",
    "# Get the current folder where the script is running\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Find source and destination folders dynamically\n",
    "source_folder = find_folder(current_dir, source_folder_name)\n",
    "destination_folder = find_folder(current_dir, destination_folder_name)\n",
    "\n",
    "if not source_folder:\n",
    "    print(f\"Source folder '{source_folder_name}' not found.\")\n",
    "elif not destination_folder:\n",
    "    print(f\"Destination folder '{destination_folder_name}' not found.\")\n",
    "else:\n",
    "    # Move files\n",
    "    csv_moved, json_moved = move_csv_and_json_files(source_folder, destination_folder)\n",
    "\n",
    "    # Delete subfolders\n",
    "    subfolders_deleted = delete_subfolders_in_source(source_folder)\n",
    "\n",
    "    # Remove duplicate files\n",
    "    duplicate_csv_deleted, duplicate_json_deleted = remove_duplicate_files(destination_folder)\n",
    "\n",
    "    # Final Output\n",
    "    print(f\"CSV Files Moved: {csv_moved}\")\n",
    "    print(f\"JSON Files Moved: {json_moved}\")\n",
    "    print(f\"Source Sub Folders Deleted: {subfolders_deleted}\")\n",
    "    print(f\"Duplicate CSVs Deleted: {duplicate_csv_deleted}\")\n",
    "    print(f\"Duplicate JSONs Deleted: {duplicate_json_deleted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consolidate CSV & JSON Files with Similar Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garmin UDSFile Consoldiation - Many JSON to One - Consolidated WhoopDate -> CleanWhoopData\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# ----------- Configuration Section -----------\n",
    "source_folder_name = \"ConsolidatedGarminData\"\n",
    "destination_folder_name = \"CleanGarminData\"\n",
    "keyword_in_path = \"UDSFile\"  # ðŸ”¹ Only JSON files with this keyword in the FILENAME\n",
    "output_consolidated_filename = \"Consolidated_UDSFile.json\"\n",
    "# ----------------------------------------------\n",
    "\n",
    "def find_folder(base_folder, target_folder_name):\n",
    "    \"\"\"Recursively find a folder with a specific name.\"\"\"\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        if target_folder_name in dirs:\n",
    "            return os.path.join(root, target_folder_name)\n",
    "    return None\n",
    "\n",
    "def consolidate_json_files(source_folder, keyword):\n",
    "    \"\"\"Find JSON files containing the keyword in their filename, and consolidate them.\"\"\"\n",
    "    all_records = []\n",
    "    files_to_delete = []\n",
    "    jsons_consolidated = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.json') and keyword.lower() in file.lower():\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                        if isinstance(data, dict):\n",
    "                            all_records.append(data)\n",
    "                        elif isinstance(data, list):\n",
    "                            all_records.extend(data)\n",
    "\n",
    "                    files_to_delete.append(file_path)\n",
    "                    jsons_consolidated += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return all_records, files_to_delete, jsons_consolidated\n",
    "\n",
    "def remove_duplicate_records(records):\n",
    "    \"\"\"Remove duplicate records by hashing JSON strings.\"\"\"\n",
    "    seen = set()\n",
    "    unique_records = []\n",
    "    duplicates_removed = 0\n",
    "\n",
    "    for record in records:\n",
    "        record_str = json.dumps(record, sort_keys=True)  # Serialize to a JSON string\n",
    "\n",
    "        if record_str not in seen:\n",
    "            seen.add(record_str)\n",
    "            unique_records.append(record)\n",
    "        else:\n",
    "            duplicates_removed += 1\n",
    "\n",
    "    return unique_records, duplicates_removed\n",
    "\n",
    "def move_final_json(destination_folder, output_filename, consolidated_records):\n",
    "    \"\"\"Save consolidated unique records into final JSON file in destination folder.\"\"\"\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    final_path = os.path.join(destination_folder, output_filename)\n",
    "\n",
    "    with open(final_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(consolidated_records, f, indent=4)\n",
    "\n",
    "def delete_source_jsons(files_to_delete):\n",
    "    \"\"\"Delete original JSON files that were consolidated.\"\"\"\n",
    "    for file_path in files_to_delete:\n",
    "        os.remove(file_path)\n",
    "    return len(files_to_delete)\n",
    "\n",
    "# ----------- Main Execution -----------\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "source_folder = find_folder(current_dir, source_folder_name)\n",
    "destination_folder = find_folder(current_dir, destination_folder_name)\n",
    "\n",
    "if not source_folder:\n",
    "    print(f\"Source folder '{source_folder_name}' not found.\")\n",
    "elif not destination_folder:\n",
    "    print(f\"Destination folder '{destination_folder_name}' not found.\")\n",
    "else:\n",
    "    all_records, files_to_delete, jsons_consolidated = consolidate_json_files(source_folder, keyword_in_path)\n",
    "\n",
    "    consolidated_records, duplicates_removed = remove_duplicate_records(all_records)\n",
    "    total_records = len(consolidated_records)\n",
    "\n",
    "    move_final_json(destination_folder, output_consolidated_filename, consolidated_records)\n",
    "    jsons_deleted = delete_source_jsons(files_to_delete)\n",
    "\n",
    "    # Final Output\n",
    "    print(f\"JSONs Consolidated: {jsons_consolidated}\")\n",
    "    print(f\"Source JSONs Deleted: {jsons_deleted}\")\n",
    "    print(f\"Duplicate Records Removed: {duplicates_removed}\")\n",
    "    print(f\"Total Records Consolidated: {total_records}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONs Consolidated: 15\n",
      "Source JSONs Deleted: 15\n",
      "Duplicate Records Removed: 21\n",
      "Total Records Consolidated: 2346\n"
     ]
    }
   ],
   "source": [
    "# Garmin TrainingHistory Consoldiation - Many JSON to One - Consolidated WhoopDate -> CleanWhoopData\n",
    "\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# ----------- Configuration Section -----------\n",
    "source_folder_name = \"ConsolidatedGarminData\"\n",
    "destination_folder_name = \"CleanGarminData\"\n",
    "keyword_in_path = \"TrainingHistory\"  # ðŸ”¹ Only JSON files with this keyword in the FILENAME\n",
    "output_consolidated_filename = \"Consolidated_TrainingHistory.json\"\n",
    "# ----------------------------------------------\n",
    "\n",
    "def find_folder(base_folder, target_folder_name):\n",
    "    \"\"\"Recursively find a folder with a specific name.\"\"\"\n",
    "    for root, dirs, files in os.walk(base_folder):\n",
    "        if target_folder_name in dirs:\n",
    "            return os.path.join(root, target_folder_name)\n",
    "    return None\n",
    "\n",
    "def consolidate_json_files(source_folder, keyword):\n",
    "    \"\"\"Find JSON files containing the keyword in their filename, and consolidate them.\"\"\"\n",
    "    all_records = []\n",
    "    files_to_delete = []\n",
    "    jsons_consolidated = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(source_folder):\n",
    "        for file in files:\n",
    "            if file.lower().endswith('.json') and keyword.lower() in file.lower():\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                        data = json.load(f)\n",
    "\n",
    "                        if isinstance(data, dict):\n",
    "                            all_records.append(data)\n",
    "                        elif isinstance(data, list):\n",
    "                            all_records.extend(data)\n",
    "\n",
    "                    files_to_delete.append(file_path)\n",
    "                    jsons_consolidated += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return all_records, files_to_delete, jsons_consolidated\n",
    "\n",
    "def remove_duplicate_records(records):\n",
    "    \"\"\"Remove duplicate records by hashing JSON strings.\"\"\"\n",
    "    seen = set()\n",
    "    unique_records = []\n",
    "    duplicates_removed = 0\n",
    "\n",
    "    for record in records:\n",
    "        record_str = json.dumps(record, sort_keys=True)  # Serialize to a JSON string\n",
    "\n",
    "        if record_str not in seen:\n",
    "            seen.add(record_str)\n",
    "            unique_records.append(record)\n",
    "        else:\n",
    "            duplicates_removed += 1\n",
    "\n",
    "    return unique_records, duplicates_removed\n",
    "\n",
    "def move_final_json(destination_folder, output_filename, consolidated_records):\n",
    "    \"\"\"Save consolidated unique records into final JSON file in destination folder.\"\"\"\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    final_path = os.path.join(destination_folder, output_filename)\n",
    "\n",
    "    with open(final_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(consolidated_records, f, indent=4)\n",
    "\n",
    "def delete_source_jsons(files_to_delete):\n",
    "    \"\"\"Delete original JSON files that were consolidated.\"\"\"\n",
    "    for file_path in files_to_delete:\n",
    "        os.remove(file_path)\n",
    "    return len(files_to_delete)\n",
    "\n",
    "# ----------- Main Execution -----------\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "source_folder = find_folder(current_dir, source_folder_name)\n",
    "destination_folder = find_folder(current_dir, destination_folder_name)\n",
    "\n",
    "if not source_folder:\n",
    "    print(f\"Source folder '{source_folder_name}' not found.\")\n",
    "elif not destination_folder:\n",
    "    print(f\"Destination folder '{destination_folder_name}' not found.\")\n",
    "else:\n",
    "    all_records, files_to_delete, jsons_consolidated = consolidate_json_files(source_folder, keyword_in_path)\n",
    "\n",
    "    consolidated_records, duplicates_removed = remove_duplicate_records(all_records)\n",
    "    total_records = len(consolidated_records)\n",
    "\n",
    "    move_final_json(destination_folder, output_consolidated_filename, consolidated_records)\n",
    "    jsons_deleted = delete_source_jsons(files_to_delete)\n",
    "\n",
    "    # Final Output\n",
    "    print(f\"JSONs Consolidated: {jsons_consolidated}\")\n",
    "    print(f\"Source JSONs Deleted: {jsons_deleted}\")\n",
    "    print(f\"Duplicate Records Removed: {duplicates_removed}\")\n",
    "    print(f\"Total Records Consolidated: {total_records}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
